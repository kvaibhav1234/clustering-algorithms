# -*- coding: utf-8 -*-
"""clustering

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r7cWDtF8FunLjOamBfnyAthaeLHr8x9L
"""

import random
from re import U 

import numpy as np 

from sklearn.decomposition import PCA 
"""
#Variables - Clustering 
N = 40000  #edges = n*dl = m*dr = N
k = 100    #num of clusters ---> n = k
#Variables - Bipartite Graph 
dl = 400     #degree left side
dr = 40      #degree right side
n = k        #left side 
m = 1000     #right side 
"""
N = 400000
k = 50000
dl = 8 
dr = 10 
n = k 
m = 40000


data = np.zeros((N, n+m))


right_deg = np.array([dr]*m)
left_deg = np.array([dl]*m)

#print("Left Degree:", left_deg)
#print("Right Degree:", right_deg)


for i in range(N): 


  x = np.random.choice(m, p=right_deg / np.sum(right_deg))

  #print("y, x+N: ", y,x+m)

  y = np.random.choice(n, p=left_deg / np.sum(left_deg))

  data[i][x+n] = 1
  data[i][y] = 1

  right_deg[x] -= 1
  left_deg[y] -= 1


  """
  row = np.array([N*d])
  data = np.append(right_deg,[row],axis=0)

  col = np.array([Nd*2*N+1])
  data = np.append(left_deg,[col],axis=0)
  """


  filename = "Dataset 400000"
  np.savetxt(filename + ".txt", data, fmt = '%u', delimiter=" ")


  #### Dimensionality reduction
  """
  PCA_dim = 60 
  reduced_data = PCA(n_components=PCA_dim).fit_transform(data[:. 0:-1])

  filename = "reduced_dataset_N1200_dim60"
  np.savetxt(filename + ".txt", np.concatenate((reduced_data, (data[:,-1]).reshape(-1,1)), axis =1), delimiter=" ")

  ####
  Dimensionality reduction 
  PCA_dim = 90 
  reduced_data = PCA(n_components= PCA_dim).fit_transform(data[:,-1])

  filename = "reduced_dataset_N1200_dim90"
  np.savetxt(filename + ".txt", np.concatenate((reduced_data, (data[:, -1]).reshape(-1,1)), axis =1), delimiter= " ")



  #print("Dataset:", data)


  """