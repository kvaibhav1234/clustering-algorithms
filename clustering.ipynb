{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# label the clusters: Maximum weighted matching problem using Hungarian Algorithm\n",
        "\n",
        "\"\"\"\n",
        "input: \n",
        "cluster_labels - labels after running k-means\n",
        "true_labels - labels from the dataset\n",
        "k - number of distinct labels\n",
        "number_samples - number of datapoints/samples\n",
        "\n",
        "Returns:\n",
        "cluster_labels - predicted labels to each datapoint\n",
        "\"\"\"\n",
        "\n",
        "import dlib\n",
        "import numpy as np\n",
        "\n",
        "def labelling(cluster_labels, true_labels, k, number_samples):\n",
        "\n",
        "    matching = np.zeros((k, k), dtype = int)\n",
        "\n",
        "    for i in range(number_samples):\n",
        "        matching[cluster_labels[i]][true_labels[i]] += 1\n",
        "\n",
        "    assignment = dlib.max_cost_assignment(dlib.matrix(matching))\n",
        "\n",
        "    for i in range(number_samples):\n",
        "        cluster_labels[i] = assignment[cluster_labels[i]]\n",
        "\n",
        "    return cluster_labels\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "def accuracy_analysis(accuracy_list):\n",
        "\taverage = np.average(accuracy_list)\n",
        "\tstandard_deviation = np.std(accuracy_list)\n",
        "\tmaximum = max(accuracy_list)\n",
        "\n",
        "\tprint(\"Accuracy Analysis: Average = \", average, \" ; Standard Deviation = \", standard_deviation, \"; Best Accuracy = \", maximum)\n",
        "\n",
        "\treturn None"
      ],
      "metadata": {
        "id": "UHrdRzcM1HTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_digits       #This dataset contains handwritten digits from 0 to 9\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#\"LOAD THE DATASET\" CODE\n",
        "\n",
        "#data, labels = load_digits(return_X_y=True) \n",
        "#(n_samples, n_features, n_digits = data.shape, np.unique(labels).size)\n",
        "#print(f\"# digits: {n_digits}; # samples: {n_samples}; # features {n_features}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data, labels = load_digits(return_X_y=True)  #first kept returning bunch, so I changed \"digits\" to \"data, labels\" and made it True, not False\n",
        "(n_samples, n_features),n_clusters = data.shape, np.unique(labels).size\n",
        "print(f\"# digits: {n_clusters}; # samples: {n_samples}; # features {n_features}\")\n",
        "\n",
        "\n",
        "#classes = clusters = k = 10\n",
        "\n",
        "accuracy_list = []\n",
        "no_epochs = 5\n",
        "\n",
        "for _ in range(no_epochs):\n",
        "\t\n",
        "  #K-Means: \n",
        "  kmeans = KMeans(n_clusters=10, init='random',n_init=1, max_iter=30).fit(data)  #only this line gets changed when switching methods - WORKS \n",
        " \n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "predicted_labels = labelling(kmeans.labels_, labels, n_clusters, n_samples)\n",
        "accuracy = accuracy_score(labels, predicted_labels)\n",
        "accuracy_list.append(accuracy)\n",
        "\n",
        "accuracy_analysis(accuracy_list)\n",
        "\n",
        "#K-Means: \n",
        "kmeans.labels_\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Comparing different clustering algorithms on toy datasets: https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html\n",
        "Main Clustering Website: https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering \n",
        "Learn about load_digits dataset: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits \n",
        "k-means on load_digits dataset: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html \n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "4bUyFJLlGhMD",
        "outputId": "ab113105-95be-4502-af98-17d01eb84b46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# digits: 10; # samples: 1797; # features 64\n",
            "Accuracy Analysis: Average =  0.7640511964385086  ; Standard Deviation =  0.0 ; Best Accuracy =  0.7640511964385086\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nComparing different clustering algorithms on toy datasets: https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html\\nMain Clustering Website: https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering \\nLearn about load_digits dataset: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits \\nk-means on load_digits dataset: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering \n",
        "import numpy as np\n",
        "from sklearn.datasets import load_digits       #This dataset contains handwritten digits from 0 to 9\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "#\"LOAD THE DATASET\" CODE\n",
        "\n",
        "#data, labels = load_digits(return_X_y=True) \n",
        "#(n_samples, n_features, n_digits = data.shape, np.unique(labels).size)\n",
        "#print(f\"# digits: {n_digits}; # samples: {n_samples}; # features {n_features}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data, labels = load_digits(return_X_y=True)  #first kept returning bunch, so I changed \"digits\" to \"data, labels\" and made it True, not False\n",
        "(n_samples, n_features),n_clusters = data.shape, np.unique(labels).size\n",
        "print(f\"# digits: {n_clusters}; # samples: {n_samples}; # features {n_features}\")\n",
        "\n",
        "\n",
        "#classes = clusters = k = 10\n",
        "\n",
        "accuracy_list = []\n",
        "no_epochs = 5\n",
        "\n",
        "for _ in range(no_epochs):\n",
        "\t\n",
        "\n",
        " \n",
        "  #Hierarchical Clustering: \n",
        "  clustering = AgglomerativeClustering(n_clusters=10, linkage='ward').fit(data)    #WORKS \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "predicted_labels = labelling(clustering.fit_predict(data), labels, n_clusters, n_samples)\n",
        "accuracy = accuracy_score(labels, predicted_labels)\n",
        "accuracy_list.append(accuracy)\n",
        "\n",
        "accuracy_analysis(accuracy_list)\n",
        "\n",
        "\n",
        "#Hierarchical Clustering: \n",
        "clustering.labels_\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMa6bmv37afm",
        "outputId": "79356880-66bc-4b28-c87d-b33099beb1d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# digits: 10; # samples: 1797; # features 64\n",
            "Accuracy Analysis: Average =  0.8402893711741792  ; Standard Deviation =  0.0 ; Best Accuracy =  0.8402893711741792\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 8, ..., 8, 3, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_digits       #This dataset contains handwritten digits from 0 to 9\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "data, labels = load_digits(return_X_y=True)  #first kept returning bunch, so I changed \"digits\" to \"data, labels\" and made it True, not False\n",
        "(n_samples, n_features),n_clusters = data.shape, np.unique(labels).size\n",
        "print(f\"# digits: {n_clusters}; # samples: {n_samples}; # features {n_features}\")\n",
        "\n",
        "\n",
        "#classes = clusters = k = 10\n",
        "\n",
        "accuracy_list = []\n",
        "no_epochs = 5\n",
        "\n",
        "for _ in range(no_epochs):\n",
        "\t\n",
        "  \n",
        "\n",
        "  #DBSCAN: \n",
        "  #clustering = DBSCAN(eps=200000, min_samples=20).fit(data)  \n",
        "  #clustering = DBSCAN(eps=0.00000000000001, min_samples=50).fit(data)\n",
        "  #clustering = DBSCAN(eps=0.5, min_samples=10).fit(data)\n",
        "  #clustering = DBSCAN(eps=0.001, min_samples=40).fit(data)\n",
        "  #clustering = DBSCAN(eps=1.00002, min_samples=344444).fit(data)\n",
        "  #clustering = DBSCAN(eps=10, min_samples=50).fit(data)\n",
        "  #clustering = DBSCAN(eps=30, min_samples=50).fit(data)     ------------------------> 13%\n",
        "  #clustering = DBSCAN(eps=20, min_samples=50).fit(data)     ------------------------> 19%\n",
        "  #clustering = DBSCAN(eps=40, min_samples=50).fit(data)\n",
        "  #clustering = DBSCAN(eps=30, min_samples=60).fit(data)     ------------------------> 22%\n",
        "  #clustering = DBSCAN(eps=30, min_samples=90).fit(data)      ------------------------> 37%\n",
        "  clustering = DBSCAN(eps=30, min_samples=85).fit(data)       #-------------------------> 41%\n",
        "  #clustering = DBSCAN(eps=25, min_samples=85).fit(data)       ---------------------------> 27%\n",
        "  #clustering = DBSCAN(eps=30, min_samples=70).fit(data)       ----------------------------> 21%\n",
        "  #clustering = DBSCAN(eps=60, min_samples=80).fit(data) \n",
        "\n",
        "\n",
        "predicted_labels = labelling(clustering.fit_predict(data), labels, n_clusters, n_samples)\n",
        "accuracy = accuracy_score(labels, predicted_labels)\n",
        "accuracy_list.append(accuracy)\n",
        "\n",
        "accuracy_analysis(accuracy_list)\n",
        "\n",
        "#DBSCAN: \n",
        "clustering.labels_\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVbHJfHYG0QQ",
        "outputId": "504ac47a-a0d5-4bd0-d6bf-be6a71044c70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# digits: 10; # samples: 1797; # features 64\n",
            "Accuracy Analysis: Average =  0.41457985531441294  ; Standard Deviation =  0.0 ; Best Accuracy =  0.41457985531441294\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 5, ..., 1, 0, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_digits       #This dataset contains handwritten digits from 0 to 9\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data, labels = load_digits(return_X_y=True)  #first kept returning bunch, so I changed \"digits\" to \"data, labels\" and made it True, not False\n",
        "(n_samples, n_features),n_clusters = data.shape, np.unique(labels).size\n",
        "print(f\"# digits: {n_clusters}; # samples: {n_samples}; # features {n_features}\")\n",
        "\n",
        "\n",
        "#classes = clusters = k = 10\n",
        "\n",
        "accuracy_list = []\n",
        "no_epochs = 5\n",
        "\n",
        "for _ in range(no_epochs):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #Gaussian Mixture: \n",
        "  gm = GaussianMixture(n_components=n_clusters).fit(data)\n",
        "\n",
        "\n",
        "\n",
        "predicted_labels = labelling(gm.predict(data), labels, n_clusters, n_samples)\n",
        "accuracy = accuracy_score(labels, predicted_labels)\n",
        "accuracy_list.append(accuracy)\n",
        "\n",
        "accuracy_analysis(accuracy_list)\n",
        "\n",
        "\n",
        "\n",
        "#Gaussian Mixture: <----------------------------- Error: GaussianMixture' object has no attribute 'labels_'\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ve29DpjdHOUk",
        "outputId": "eecd280e-2fd5-43cd-f94f-09feb5cf293e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# digits: 10; # samples: 1797; # features 64\n",
            "Accuracy Analysis: Average =  0.808569838619922  ; Standard Deviation =  0.0 ; Best Accuracy =  0.808569838619922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.cluster import Birch\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_digits       #This dataset contains handwritten digits from 0 to 9\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data, labels = load_digits(return_X_y=True)  #first kept returning bunch, so I changed \"digits\" to \"data, labels\" and made it True, not False\n",
        "(n_samples, n_features),n_clusters = data.shape, np.unique(labels).size\n",
        "print(f\"# digits: {n_clusters}; # samples: {n_samples}; # features {n_features}\")\n",
        "\n",
        "\n",
        "#classes = clusters = k = 10\n",
        "\n",
        "accuracy_list = []\n",
        "no_epochs = 5\n",
        "\n",
        "for _ in range(no_epochs):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #BIRCH: \n",
        "  brc = Birch(n_clusters=10).fit(data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "predicted_labels = labelling(brc.predict(data), labels, n_clusters, n_samples)\n",
        "accuracy = accuracy_score(labels, predicted_labels)\n",
        "accuracy_list.append(accuracy)\n",
        "\n",
        "accuracy_analysis(accuracy_list)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShI-jNePJASm",
        "outputId": "67213ac3-c51b-452a-f6f5-0cd19c8d01e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# digits: 10; # samples: 1797; # features 64\n",
            "Accuracy Analysis: Average =  0.8402893711741792  ; Standard Deviation =  0.0 ; Best Accuracy =  0.8402893711741792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AffinityPropagation\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_digits       #This dataset contains handwritten digits from 0 to 9\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data, labels = load_digits(return_X_y=True)  #first kept returning bunch, so I changed \"digits\" to \"data, labels\" and made it True, not False\n",
        "(n_samples, n_features),n_clusters = data.shape, np.unique(labels).size\n",
        "print(f\"# digits: {n_clusters}; # samples: {n_samples}; # features {n_features}\")\n",
        "\n",
        "\n",
        "#classes = clusters = k = 10\n",
        "\n",
        "accuracy_list = []\n",
        "no_epochs = 5\n",
        "\n",
        "for _ in range(no_epochs):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #Affinity Propagation: \n",
        "  clustering = AffinityPropagation(max_iter=50).fit(data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "predicted_labels = labelling(clustering.fit_predict(data), labels, n_clusters, n_samples)\n",
        "accuracy = accuracy_score(labels, predicted_labels)\n",
        "accuracy_list.append(accuracy)\n",
        "\n",
        "accuracy_analysis(accuracy_list)\n",
        "\n",
        "clustering.labels_\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "t45q2X2_MmFD",
        "outputId": "fe6bfa8d-c0af-4850-fbb5-738f5f22b4d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# digits: 10; # samples: 1797; # features 64\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f2fc0b8a59d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabelling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclustering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0maccuracy_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-9f440231acba>\u001b[0m in \u001b[0;36mlabelling\u001b[0;34m(cluster_labels, true_labels, k, number_samples)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mmatching\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcluster_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrue_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0massignment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_cost_assignment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatching\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 76 is out of bounds for axis 0 with size 10"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import SpectralClustering\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_digits       #This dataset contains handwritten digits from 0 to 9\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data, labels = load_digits(return_X_y=True)  #first kept returning bunch, so I changed \"digits\" to \"data, labels\" and made it True, not False\n",
        "(n_samples, n_features),n_clusters = data.shape, np.unique(labels).size\n",
        "print(f\"# digits: {n_clusters}; # samples: {n_samples}; # features {n_features}\")\n",
        "\n",
        "\n",
        "#classes = clusters = k = 10\n",
        "\n",
        "accuracy_list = []\n",
        "no_epochs = 5\n",
        "\n",
        "for _ in range(no_epochs):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #Spectral Clustering: \n",
        "  clustering = SpectralClustering(n_clusters=10).fit(data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "predicted_labels = labelling(clustering.fit_predict(data), labels, n_clusters, n_samples)\n",
        "accuracy = accuracy_score(labels, predicted_labels)\n",
        "accuracy_list.append(accuracy)\n",
        "\n",
        "accuracy_analysis(accuracy_list)\n",
        "\n",
        "clustering.labels_\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV7rRZ00Nx4f",
        "outputId": "124320f8-1462-4eb8-f026-410da95bf31b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# digits: 10; # samples: 1797; # features 64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_spectral_embedding.py:261: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  \"Graph is not fully connected, spectral embedding may not work as expected.\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_spectral_embedding.py:261: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  \"Graph is not fully connected, spectral embedding may not work as expected.\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_spectral_embedding.py:261: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  \"Graph is not fully connected, spectral embedding may not work as expected.\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_spectral_embedding.py:261: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  \"Graph is not fully connected, spectral embedding may not work as expected.\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_spectral_embedding.py:261: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  \"Graph is not fully connected, spectral embedding may not work as expected.\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_spectral_embedding.py:261: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  \"Graph is not fully connected, spectral embedding may not work as expected.\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Analysis: Average =  0.10406232609905398  ; Standard Deviation =  0.0 ; Best Accuracy =  0.10406232609905398\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 1, 1, 1], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import MeanShift\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_digits       #This dataset contains handwritten digits from 0 to 9\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data, labels = load_digits(return_X_y=True)  #first kept returning bunch, so I changed \"digits\" to \"data, labels\" and made it True, not False\n",
        "(n_samples, n_features),n_clusters = data.shape, np.unique(labels).size\n",
        "print(f\"# digits: {n_clusters}; # samples: {n_samples}; # features {n_features}\")\n",
        "\n",
        "\n",
        "#classes = clusters = k = 10\n",
        "\n",
        "accuracy_list = []\n",
        "no_epochs = 5\n",
        "\n",
        "for _ in range(no_epochs):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #Mean Shift: \n",
        "  clustering = MeanShift(bandwidth=2,max_iter=300).fit(data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "predicted_labels = labelling(clustering.predict(data), labels, n_clusters, n_samples)\n",
        "accuracy = accuracy_score(labels, predicted_labels)\n",
        "accuracy_list.append(accuracy)\n",
        "\n",
        "accuracy_analysis(accuracy_list)\n",
        "\n",
        "clustering.labels_\n",
        "\n"
      ],
      "metadata": {
        "id": "h0Zs0Q-nPZJf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "f8c7ba9d-2bdb-44ff-81c9-aa47a3a6ed55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# digits: 10; # samples: 1797; # features 64\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-d91606f1be40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabelling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclustering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0maccuracy_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-9f440231acba>\u001b[0m in \u001b[0;36mlabelling\u001b[0;34m(cluster_labels, true_labels, k, number_samples)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mmatching\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcluster_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrue_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0massignment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_cost_assignment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatching\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 823 is out of bounds for axis 0 with size 10"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.cluster import BisectingKMeans\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_digits       #This dataset contains handwritten digits from 0 to 9\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "data, labels = load_digits(return_X_y=True)  #first kept returning bunch, so I changed \"digits\" to \"data, labels\" and made it True, not False\n",
        "(n_samples, n_features),n_clusters = data.shape, np.unique(labels).size\n",
        "print(f\"# digits: {n_clusters}; # samples: {n_samples}; # features {n_features}\")\n",
        "\n",
        "\n",
        "#classes = clusters = k = 10\n",
        "\n",
        "accuracy_list = []\n",
        "no_epochs = 5\n",
        "\n",
        "for _ in range(no_epochs):\n",
        "\t\n",
        "  \n",
        "\n",
        "  #Bisecting KMeans: \n",
        "  bisect_means = BisectingKMeans(n_clusters=10).fit(data)\n",
        "\n",
        "\n",
        "\n",
        "predicted_labels = labelling(bisect_means.predict(data), labels, n_clusters, n_samples)\n",
        "accuracy = accuracy_score(labels, predicted_labels)\n",
        "accuracy_list.append(accuracy)\n",
        "\n",
        "accuracy_analysis(accuracy_list)\n",
        "\n",
        "#Bisecting KMeans: \n",
        "bisect_means.labels_\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "2r6ue_NRSJuW",
        "outputId": "70c89623-7647-41a2-9ca5-388b671b5314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-a376852534f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBisectingKMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_digits\u001b[0m       \u001b[0;31m#This dataset contains handwritten digits from 0 to 9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'BisectingKMeans' from 'sklearn.cluster' (/usr/local/lib/python3.7/dist-packages/sklearn/cluster/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.cluster import OPTICS\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_digits       #This dataset contains handwritten digits from 0 to 9\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data, labels = load_digits(return_X_y=True)  #first kept returning bunch, so I changed \"digits\" to \"data, labels\" and made it True, not False\n",
        "(n_samples, n_features),n_clusters = data.shape, np.unique(labels).size\n",
        "print(f\"# digits: {n_clusters}; # samples: {n_samples}; # features {n_features}\")\n",
        "\n",
        "\n",
        "#classes = clusters = k = 10\n",
        "\n",
        "accuracy_list = []\n",
        "no_epochs = 5\n",
        "\n",
        "for _ in range(no_epochs):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #BIRCH: \n",
        "  clustering = OPTICS().fit(data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "predicted_labels = labelling(clustering.fit_predict(data), labels, n_clusters, n_samples)\n",
        "accuracy = accuracy_score(labels, predicted_labels)\n",
        "accuracy_list.append(accuracy)\n",
        "\n",
        "accuracy_analysis(accuracy_list)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "tXGTxwJURLr2",
        "outputId": "139cc0d2-50ff-47bf-fd43-afc9306acda9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# digits: 10; # samples: 1797; # features 64\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-ef63162b08b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabelling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclustering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0maccuracy_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-9f440231acba>\u001b[0m in \u001b[0;36mlabelling\u001b[0;34m(cluster_labels, true_labels, k, number_samples)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mmatching\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcluster_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrue_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0massignment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_cost_assignment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatching\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 14 is out of bounds for axis 0 with size 10"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO-Do: \n",
        "\n",
        "1) Matching: inputs n and d\n",
        "- https://towardsdatascience.com/how-to-match-two-people-with-python-7583b51ff3f9 \n",
        "\n",
        "2) Graph Theory: Definitions of vertices, edges, degree\n",
        "\n",
        "6 Algorithms that Work: \n",
        "-KMeans\n",
        "-AgglomerativeClustering (Hierarchical)\n",
        "-DBSCAN\n",
        "-Gaussian Mixture\n",
        "-Birch\n",
        "-Spectral Clustering"
      ],
      "metadata": {
        "id": "BWStDqh0__zq"
      }
    }
  ]
}